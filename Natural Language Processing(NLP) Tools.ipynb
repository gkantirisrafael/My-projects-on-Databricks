{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f7e3723-485e-48a0-9f20-b65657b00b5e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('nlp_tools').getOrCreate()\n",
    "from pyspark.ml.feature import Tokenizer,RegexTokenizer\n",
    "from pyspark.sql.functions import col,udf\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3cf78a2e-bd2a-4e69-bbe8-a6d8432a4586",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#We will create a data-frame full of sentences\n",
    "sen_def = spark.createDataFrame([\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa9624e4-d4da-48a4-9660-933e9c31ecda",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n| id|            sentence|\n+---+--------------------+\n|  0|Hi I heard about ...|\n|  1|I wish Java could...|\n|  2|Logistic,regressi...|\n+---+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "sen_def.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "163c1718-b83d-4b7b-928e-a35c3e0a4bd3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#We will create our Tokenizer objects\n",
    "tokenizer = Tokenizer(inputCol='sentence',outputCol='words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bcc1d6b3-a28a-47e3-b850-7a4a5933cee2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Regular expression tokenizer\n",
    "regex_tokenizer = RegexTokenizer(inputCol='sentence',outputCol='words',pattern='\\\\W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b767c7d-1ec2-4e11-8e68-e2d569a6375c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#user define function\n",
    "count_tokens = udf(lambda words:len(words),IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4adbec0-f5f2-49a3-b01b-4ab4e37a32fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tokenized = tokenizer.transform(sen_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61362e5c-7c5c-49b9-8e9c-cf5692c85905",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+------+\n| id|            sentence|               words|tokens|\n+---+--------------------+--------------------+------+\n|  0|Hi I heard about ...|[hi, i, heard, ab...|     5|\n|  1|I wish Java could...|[i, wish, java, c...|     7|\n|  2|Logistic,regressi...|[logistic,regress...|     1|\n+---+--------------------+--------------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "tokenized.withColumn('tokens',count_tokens(col('words'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4a43820-bb6e-4b16-9c6a-9360e524d6f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rg_tokenized = regex_tokenizer.transform(sen_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f411d3f4-5c9d-45ad-9312-7b3a590abff0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+------+\n| id|            sentence|               words|tokens|\n+---+--------------------+--------------------+------+\n|  0|Hi I heard about ...|[hi, i, heard, ab...|     5|\n|  1|I wish Java could...|[i, wish, java, c...|     7|\n|  2|Logistic,regressi...|[logistic, regres...|     5|\n+---+--------------------+--------------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "rg_tokenized.withColumn('tokens',count_tokens(col('words'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7bbaae8-7502-4030-8810-56ec8222d378",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#some words dont need to be considers , we can use a function to not count them\n",
    "#We filter common words like: I,the,had,a,etc.   Or add additional stop words.\n",
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3305fd3-bf11-429b-940a-ee2963fef3f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sentenceDataFrame = spark.createDataFrame([\n",
    "    (0,['I','saw','the','green','horse']),\n",
    "    (1,['Mary','had','a','little','lamb'])\n",
    "],['id','tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d05fa20-be0d-4392-ac9a-2504c5c362cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "remover = StopWordsRemover(inputCol='tokens',outputCol='filtered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d64d246-0761-496a-bf37-f4f433c8e3a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n| id|              tokens|            filtered|\n+---+--------------------+--------------------+\n|  0|[I, saw, the, gre...| [saw, green, horse]|\n|  1|[Mary, had, a, li...|[Mary, little, lamb]|\n+---+--------------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "remover.transform(sentenceDataFrame).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b1ec964-0d8c-442b-b77d-58c70e4b957b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#n-grams  \n",
    "from pyspark.ml.feature import NGram\n",
    "word_dataframe = spark.createDataFrame([\n",
    "    (0,['Hi','I','heard','about','Spark']),\n",
    "    (1,['I','wish','Java','could','use','case','classes']),\n",
    "    (2,['Logistic','regression','models','are','neat'])\n",
    "],['id','words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64980490-1d07-44a6-84f8-6e8a2baccf77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ngram = NGram(n=3,inputCol='words',outputCol='grams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d141cad2-87a0-4ddf-b95d-aa300a5b8171",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------+\n|grams                                                                           |\n+--------------------------------------------------------------------------------+\n|[Hi I heard, I heard about, heard about Spark]                                  |\n|[I wish Java, wish Java could, Java could use, could use case, use case classes]|\n|[Logistic regression models, regression models are, models are neat]            |\n+--------------------------------------------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "ngram.transform(word_dataframe).select('grams').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d63d350c-e809-445c-b181-1424ff2a2cc6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#############################  PART 2  ###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b16014c4-4150-44be-a825-0bd5c5e7a652",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF,IDF,Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3932002d-a548-4aad-ada7-cf08cd5ad058",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sentenceData = spark.createDataFrame([\n",
    "    (0.0,'Hi I heard about Spark'),\n",
    "    (0.0,'I wish Java could use case classes'),\n",
    "    (1.0,'Logistic regression models are neat')\n",
    "],['label','sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5ef45ab-4167-4e19-9e00-e1fe5f4e539c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n|label|            sentence|\n+-----+--------------------+\n|  0.0|Hi I heard about ...|\n|  0.0|I wish Java could...|\n|  1.0|Logistic regressi...|\n+-----+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "sentenceData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1ffce02-d4b9-41c8-870e-9496550990ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#now we will tokenize it\n",
    "tokenizer1 = Tokenizer(inputCol='sentence',outputCol='words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92d7717f-69f8-4ecb-8bf0-8312538c3d42",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "words_data = tokenizer1.transform(sentenceData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ddffe66-852b-44d4-9772-428c203e92e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------------------+------------------------------------------+\n|label|sentence                           |words                                     |\n+-----+-----------------------------------+------------------------------------------+\n|0.0  |Hi I heard about Spark             |[hi, i, heard, about, spark]              |\n|0.0  |I wish Java could use case classes |[i, wish, java, could, use, case, classes]|\n|1.0  |Logistic regression models are neat|[logistic, regression, models, are, neat] |\n+-----+-----------------------------------+------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "words_data.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90b5dd70-3f0e-4ee4-9187-937d544bfb8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "hashing_tf = HashingTF(inputCol='words',outputCol='rawFeatures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f7e77eb-dfa5-439e-9c08-01de669765fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "featurised_data = hashing_tf.transform(word_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78ac12a5-aa94-40ae-b452-d524db0f881c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "idf = IDF(inputCol='rawFeatures',outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39925567-350e-4fd3-a269-bded66082189",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "idf_model = idf.fit(featurised_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f57d5cd-a89d-4647-95f1-c59766c0b0d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rescaled_data = idf_model.transform(featurised_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "453fae1d-91c1-43ba-9255-f2afb38a0652",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|label|features                                                                                                                                                                                      |\n+-----+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|0.0  |(262144,[18700,19036,33808,66273,173558],[0.6931471805599453,0.28768207245178085,0.6931471805599453,0.6931471805599453,0.6931471805599453])                                                   |\n|0.0  |(262144,[19036,20719,55551,58672,98717,109547,192310],[0.28768207245178085,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453])|\n|1.0  |(262144,[46243,58267,91006,160975,190884],[0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453,0.6931471805599453])                                                   |\n+-----+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "rescaled_data.select('label','features').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c834c207-1f58-42de-8b1e-58bd550f8579",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Now it is in form of 'label' and 'features' and we can deal with it with any supervised learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b71119a2-dce9-4471-9335-bc8b20ab538a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f5eb228-df55-40b6-a65a-a6dfb69352ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (0,'a b c'.split(' ')),\n",
    "    (1,'a b b c a'.split(' '))\n",
    "],['id','words count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ba77548-6c86-4961-90de-f8bedd3433a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+\n| id|    words count|\n+---+---------------+\n|  0|      [a, b, c]|\n|  1|[a, b, b, c, a]|\n+---+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2cabf46-8183-4d67-a3ed-187f41a69e68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(inputCol='words count',outputCol='features',vocabSize=3,minDF=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe3c2eb5-e409-45c8-bfff-99e2b5541b1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model = cv.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e0eafd0-7c03-45e7-aff5-e56fe2d9a533",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "750da660-b2ac-4cda-8097-2e4dee6f0f8f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+-------------------------+\n|id |words count    |features                 |\n+---+---------------+-------------------------+\n|0  |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n|1  |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n+---+---------------+-------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a2a5a4c-5f36-4f05-8f56-9f774063386b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Natural Language Processing(NLP) Tools",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
